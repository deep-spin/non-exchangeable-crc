{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"results/processed.npy\"\n",
    "conf_scores, f1_scores = np.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"results/predictions.json\"\n",
    "with open(filename, \"r\") as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, answer):\n",
    "    \"\"\"Compute F1 score between prediction tokens and ground truth tokens.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    answer_tokens = normalize_answer(answer).split()\n",
    "    common = (collections.Counter(prediction_tokens) &\n",
    "              collections.Counter(answer_tokens))\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(answer_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def metric_max_over_answers(metric_fn, prediction, answer_set):\n",
    "    \"\"\"Return the maximum score between any (prediction, answer) pair.\"\"\"\n",
    "    max_score = -float(\"inf\")\n",
    "    for answer in answer_set:\n",
    "        score = metric_fn(prediction, answer)\n",
    "        max_score = max(max_score, score)\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs = 100\n",
    "answers_list = []\n",
    "questions_list = []\n",
    "dataset = []\n",
    "\n",
    "for entry in tqdm.tqdm(raw_data, \"processing data...\"):\n",
    "    # Get questions.\n",
    "    questions = entry[\"question\"]\n",
    "    questions_list.append(questions)\n",
    "\n",
    "    # Get answers.\n",
    "    answers = entry[\"gold_answers\"]\n",
    "    answers_list.append(answers)\n",
    "\n",
    "    # Get predictions.\n",
    "    predictions = None\n",
    "    for p in entry[\"predictions\"]:\n",
    "        if p[\"top_k\"] == top_docs:\n",
    "            predictions = p[\"predictions\"]\n",
    "            break\n",
    "    if predictions is None:\n",
    "        raise RuntimeError(f\"Could not find entry corresponding to top_k={top_docs}.\")\n",
    "\n",
    "    # Score predictions by F1, and sort by joint doc * span confidence score.\n",
    "    deduped = set()\n",
    "    scored_predictions = []\n",
    "    for p in predictions:\n",
    "        if normalize_answer(p[\"text\"]) in deduped:\n",
    "            continue\n",
    "        deduped.add(normalize_answer(p[\"text\"]))\n",
    "        f1 = metric_max_over_answers(f1_score, p[\"text\"], answers)\n",
    "        scored_predictions.append((p[\"score\"] + p[\"relevance_score\"], f1))\n",
    "    scored_predictions = sorted(scored_predictions, key=lambda x: -x[0])\n",
    "\n",
    "    # Store in dataset as ordered nested set.\n",
    "    conf_scores, pred_scores = zip(*scored_predictions)\n",
    "    set_scores = np.maximum.accumulate(pred_scores).tolist()\n",
    "    dataset.append((conf_scores, set_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save questions_list\n",
    "with open(\"results/questions_list\", \"wb\") as fp:\n",
    "    pickle.dump(questions_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load questions_list\n",
    "with open(\"results/questions_list\", \"rb\") as fp:\n",
    "    questions_list = pickle.load(fp)\n",
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "query_embedding = model.encode(questions_list)\n",
    "passage_embedding = model.encode(questions_list)\n",
    "\n",
    "# similarity\n",
    "W_all = util.dot_score(query_embedding, query_embedding).numpy() # [3610, 3610]\n",
    "\n",
    "# normalize\n",
    "min_value = np.min(W_all)\n",
    "max_value = np.max(W_all)\n",
    "W_all = (W_all - min_value) / (max_value - min_value)\n",
    "\n",
    "# save W_all\n",
    "np.save(\"results/w-normalized-multi-qa-mpnet-base-dot-v1\", W_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
