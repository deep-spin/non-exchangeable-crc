{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the code from Barber et al. (2023) available at https://rinafb.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(98765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_model(X_train, Y_train_l,mtype='logreg'):\n",
    "    n_inputs = X_train.shape[-1]\n",
    "    n_outputs = Y_train_l.shape[-1]\n",
    "    X_train = X_train.reshape(-1,n_inputs)\n",
    "    Y_train = Y_train_l.reshape(-1,n_outputs)\n",
    "    \n",
    "    if mtype =='logreg':\n",
    "        model = MultiOutputClassifier(estimator= LogisticRegression())\n",
    "        \n",
    "    elif mtype=='svm':\n",
    "        Y_train = np.argmax(Y_train,axis=1)\n",
    "        model = svm.SVC()\n",
    "        \n",
    "    model.fit(X_train,Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split conformal prediction with logistic regression.\n",
    "def splitCP(X, Y, x, nclasses, alpha, weights=[], lambdas_probs=False):\n",
    "    # weights are used for computing quantiles for the prediction interval\n",
    "    n = len(Y)\n",
    "    loss_beta = 1.0\n",
    "    loss_alpha = 0.0\n",
    "    \n",
    "    if (len(weights)==0):\n",
    "        weights = np.ones(n+1)\n",
    "    if (len(weights)==n):\n",
    "        weights = np.r_[weights,1]\n",
    "    \n",
    "    # odd data points for training, even for calibration\n",
    "    inds_odd = np.arange(1, int(np.ceil(n/2)*2-1), 2)\n",
    "    inds_even = np.arange(2, int(np.floor(n/2)*2), 2)\n",
    "    \n",
    "    # train model\n",
    "    model = get_model(X[inds_odd], Y[inds_odd], 'logreg')\n",
    "    \n",
    "    if lambdas_probs:\n",
    "        lambdas = np.linspace(0, 1, 50) # probability threshold\n",
    "    else:\n",
    "        lambdas = np.arange(0, nclasses+1, 1)\n",
    "    \n",
    "    # compute residual quantile on calibration set\n",
    "    n_w = np.sum(weights[inds_even])\n",
    "    r_hats = np.zeros(len(lambdas))\n",
    "\n",
    "    Ytrue = Y[inds_even] # num_points x num_classes\n",
    "    \n",
    "    predictions = np.array(model.predict_proba(X[inds_even]))[:,:,1] # num_classes x num_points\n",
    "    losses = np.zeros((len(lambdas), len(predictions.T)))\n",
    "    for li,l in enumerate(lambdas):\n",
    "        if lambdas_probs:\n",
    "            fn = np.logical_and(predictions.T < 1-l, Ytrue)\n",
    "            ind = np.nonzero(np.sum(Ytrue, 1) > 0)[0]\n",
    "            fnr = np.zeros(len(predictions.T))\n",
    "            fnr[ind] = np.sum(fn, 1)[ind] / np.sum(Ytrue, 1)[ind]\n",
    "        else:\n",
    "            assert(False)\n",
    "        losses[li, :] = fnr\n",
    "        ws = np.sum(weights[inds_even]*losses[li])\n",
    "        r_hats[li] = 1/n_w * ws\n",
    "    calib_lambdas = (r_hats*n_w/(n_w+1)) + loss_beta/(n_w+1)\n",
    "    lambda_chosen=nclasses\n",
    "    for i,li in enumerate(calib_lambdas):\n",
    "        if li<=alpha:\n",
    "            lambda_chosen = lambdas[i]\n",
    "            break\n",
    " \n",
    "    y_pred = np.array(model.predict_proba(x.reshape(1,-1)))[:,:,1]\n",
    "\n",
    "    if lambdas_probs:\n",
    "        y_pred_chosen = np.nonzero(y_pred.T[0, :] >= 1-lambda_chosen)[0]\n",
    "    else:\n",
    "        y_pred_sorted = np.argsort(y_pred.T)[0]\n",
    "        y_pred_sorted_inv = y_pred_sorted[::-1]\n",
    "        y_pred_chosen = y_pred_sorted_inv[:lambda_chosen]\n",
    "    y_PI = np.zeros((nclasses))\n",
    "    for y in y_pred_chosen:\n",
    "        y_PI[y]=1.0\n",
    "    \n",
    "    return lambda_chosen,y_PI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for all settings\n",
    "np.random.seed(12345)\n",
    "\n",
    "N = 2000\n",
    "p = 10\n",
    "noise_level = .1\n",
    "avg_classes = 1/3\n",
    "\n",
    "from scipy.special import erfinv, erf\n",
    "bias = -0.5\n",
    "\n",
    "train_lag = 200\n",
    "ntrial = 1\n",
    "rho = 0.99\n",
    "nclasses = p\n",
    "coefficients_type = 1\n",
    "\n",
    "X = np.random.normal(size=(ntrial,N,p))\n",
    "Y = np.zeros((3,ntrial,N,nclasses))\n",
    "noise = np.random.normal(size=(ntrial,N,nclasses))\n",
    "\n",
    "# setting 1: i.i.d. data\n",
    "beta_c1 = np.eye(p) \n",
    "Y[0] = X.dot(beta_c1.T) + bias + noise_level*noise\n",
    "\n",
    "# setting 2: changepoints\n",
    "changepoints = np.r_[N//4, 3*N//4]\n",
    "n_changepoint = 2\n",
    "beta_c2 = [np.zeros_like(beta_c1) for i in range(3)]\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        beta_c2[i][:, :] = beta_c1[:, :]\n",
    "    else:\n",
    "        beta_c2[i][:, 1:] = beta_c2[i-1][:, :-1]\n",
    "        beta_c2[i][:, 0] = beta_c2[i-1][:, -1]\n",
    "\n",
    "for i in np.arange(3):\n",
    "    if i == 0:\n",
    "        ind_min = 0\n",
    "    else:\n",
    "        ind_min = changepoints[i-1]\n",
    "    if i == n_changepoint:\n",
    "        ind_max = N\n",
    "    else:\n",
    "        ind_max = changepoints[i]\n",
    "        \n",
    "    Y[1,:,ind_min:ind_max,:] = X[:,ind_min:ind_max,:].dot(beta_c2[i].T) + bias + noise[:,ind_min:ind_max,:]\n",
    "    \n",
    "# setting 3: distribution drift\n",
    "beta_c_start  = beta_c1\n",
    "beta_c_end = beta_c2[-1]\n",
    "beta_c3 = beta_c_start + np.outer(np.arange(N)/(N-1),beta_c_end-beta_c_start).reshape(-1,nclasses,p)\n",
    "for i in np.arange(N):\n",
    "    Y[2,:,i,:] = X[:,i,:].dot(beta_c3[i].T) + bias + noise[:,i,:]\n",
    "Ylabels = np.where(Y>0.0,1,0)\n",
    "\n",
    "setting_names = ['Setting 1 (i.i.d. data)',\\\n",
    "                     'Setting 2 (changepoints)','Setting 3 (distribution drift)']\n",
    "\n",
    "print(beta_c3[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4580e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run all methods\n",
    "\n",
    "PI_CP_LS = np.zeros((3,ntrial,N,nclasses))\n",
    "PI_CP_LS[:,:,:train_lag,:]=np.inf\n",
    "PI_nexCP_LS = np.copy(PI_CP_LS)\n",
    "\n",
    "PI_split_CP_LS = np.copy(PI_CP_LS)\n",
    "PI_split_nexCP_LS = np.copy(PI_CP_LS)\n",
    "lambdas_chosen = np.zeros((3,ntrial,N))\n",
    "lambdas_chosen_nex = np.zeros((3,ntrial,N))\n",
    "alpha = 0.2\n",
    "for itrial in np.arange(ntrial):\n",
    "    print(itrial)\n",
    "    for n in np.arange(train_lag,N):\n",
    "        if n%100==0:\n",
    "            print('npoint',n)\n",
    "        weights=rho**(np.arange(n,0,-1))\n",
    "        for setting in np.arange(3):\n",
    "            # CRC\n",
    "            lambdas_chosen[setting,itrial,n], PI_split_CP_LS[setting,itrial,n,:] = \\\n",
    "                splitCP(X[itrial,:n,:],Ylabels[setting,itrial,:n,:],X[itrial,n,:],nclasses,alpha,lambdas_probs=True)\n",
    "            # non-x CRC\n",
    "            lambdas_chosen_nex[setting,itrial,n], PI_split_nexCP_LS[setting,itrial,n,:] = \\\n",
    "                splitCP(X[itrial,:n,:],Ylabels[setting,itrial,:n,:],X[itrial,n,:],nclasses,alpha,lambdas_probs=True,\\\n",
    "                    weights=weights)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4fede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lambdas_chosen.max(),lambdas_chosen_nex.max())\n",
    "\n",
    "print(lambdas_chosen[1, 0, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = np.zeros((len(PI_CP_LS[train_lag:])))\n",
    "\n",
    "\n",
    "fnr_diff = Ylabels[:,:,train_lag:,:]-PI_split_CP_LS[:,:,train_lag:,:]\n",
    "fnr_idx = fnr_diff==1.0\n",
    "fnr_idx = fnr_idx.astype(int) # get false negatives\n",
    "fnr_inst = np.sum(fnr_idx,axis=-1)/np.sum(Ylabels[:,:,train_lag:,:],axis=-1) # compute fnr\n",
    "fnr_inst[np.isnan(fnr_inst)] = 0.0\n",
    "fnr_trial_crc = np.mean(fnr_inst,axis=1) # average over trials\n",
    "\n",
    "fnr_diff_nex = Ylabels[:,:,train_lag:,:]-PI_split_nexCP_LS[:,:,train_lag:,:]\n",
    "fnr_idx_nex = fnr_diff_nex==1.0\n",
    "fnr_idx_nex = fnr_idx_nex.astype(int) # get false negatives\n",
    "fnr_inst_nex1 = np.sum(fnr_idx_nex,axis=-1)\n",
    "fnr_inst_nex = fnr_inst_nex1/np.sum(Ylabels[:,:,train_lag:,:],axis=-1) # compute fnr\n",
    "fnr_inst_nex[np.isnan(fnr_inst_nex)] = 0.0\n",
    "fnr_trial_nex_crc = np.mean(fnr_inst_nex,axis=1) # average over trials\n",
    "\n",
    "    \n",
    "PI_width_crc = lambdas_chosen\n",
    "\n",
    "PI_width_nex_crc = lambdas_chosen_nex\n",
    "\n",
    "# save results\n",
    "np.savetxt('simulations_CRC_fnr_trial_'+str(alpha)+'_classes_'+str(nclasses)+'_rho_'+str(rho)+'_ntrial_'+str(ntrial)+'.txt',fnr_trial_crc)\n",
    "np.savetxt('simulations_CRC_lambdas_'+str(alpha)+'_classes_'+str(nclasses)+'_rho_'+str(rho)+'_ntrial_'+str(ntrial)+'.txt',np.mean(lambdas_chosen,axis=1))\n",
    "np.savetxt('simulations_nonxCRC_fnr_trial_'+str(alpha)+'_classes_'+str(nclasses)+'_rho_'+str(rho)+'_ntrial_'+str(ntrial)+'.txt',fnr_trial_nex_crc)\n",
    "np.savetxt('simulations_nonxCRC_lambdas_'+str(alpha)+'_classes_'+str(nclasses)+'_rho_'+str(rho)+'_ntrial_'+str(ntrial)+'.txt',np.mean(lambdas_chosen_nex,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in np.arange(3):\n",
    "    print(np.array([[setting_names[setting],'',''],\\\n",
    "        ['CRC',np.mean(fnr_trial_crc[setting]),np.mean(lambdas_chosen[setting])],\\\n",
    "        ['nonx-CRC',np.mean(fnr_trial_nex_crc[setting]),np.mean(lambdas_chosen_nex[setting])]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "window = 30 # will display a rolling average\n",
    "\n",
    "def rolling_avg(x,window):\n",
    "    return np.convolve(x,np.ones(window)/window)[(window-1):-window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "pastel_palette = sns.color_palette(\"deep\")\n",
    "font_size = 20\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 3\n",
    "\n",
    "# figure with subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 6))\n",
    "\n",
    "for i in np.arange(3):\n",
    "    sm_cov_CP_LS = rolling_avg(fnr_trial_crc[i], window)\n",
    "    sm_cov_nexCP_LS = rolling_avg(fnr_trial_nex_crc[i], window)\n",
    "\n",
    "    axes[0, i].plot(np.arange(train_lag + window, N), sm_cov_CP_LS, color=pastel_palette[1], label=\"CRC\")\n",
    "    axes[0, i].plot(np.arange(train_lag + window, N), sm_cov_nexCP_LS, color=pastel_palette[2], label=\"Non-X CRC\")\n",
    "    axes[0, i].hlines(alpha, xmin=train_lag + window, xmax=N, linestyles='--', colors='black')\n",
    "    axes[0, i].set_title(setting_names[i], fontsize=font_size)\n",
    "    if i==0:\n",
    "        axes[0, i].set_ylabel('loss', fontsize=font_size)\n",
    "    axes[0, i].set_ylim([0, 1])\n",
    "\n",
    "for i in np.arange(3):\n",
    "    sm_cov_CP_LS = rolling_avg(np.mean(lambdas_chosen[i][:, train_lag:], 0), window)\n",
    "    sm_cov_nexCP_LS = rolling_avg(np.mean(lambdas_chosen_nex[i][:, train_lag:], 0), window)\n",
    "\n",
    "    axes[1, i].plot(np.arange(train_lag + window, N), sm_cov_CP_LS, color=pastel_palette[1], label=\"CRC\")\n",
    "    axes[1, i].plot(np.arange(train_lag + window, N), sm_cov_nexCP_LS, color=pastel_palette[2], label=\"Non-X CRC\")\n",
    "    if i==0:\n",
    "        axes[1, i].set_ylabel('lambda', fontsize=font_size)\n",
    "    axes[1, i].set_xlabel('time', fontsize=font_size)\n",
    "    axes[1, i].set_ylim([0, 1])\n",
    "\n",
    "# shared legend below the subplots\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.1), fontsize=font_size)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# save plot\n",
    "plt.savefig('simulations_plot_' + str(alpha) + '_classes_' + str(nclasses) +'_rho_' + str(rho)+ '_ntrial_' + str(ntrial) +'_window_' + str(window) + '.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
